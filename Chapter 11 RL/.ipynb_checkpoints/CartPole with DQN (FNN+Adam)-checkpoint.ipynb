{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 倒立摆实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 只玩一局游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始观测：[-0.02574804 -0.0175332  -0.02261496  0.01928176]\n",
      "0: 动作 = 0\n",
      "0: 观测 = [-0.0260987  -0.21232365 -0.02222933  0.30474449], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "1: 动作 = 1\n",
      "1: 观测 = [-0.03034517 -0.01689208 -0.01613444  0.00513468], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "2: 动作 = 0\n",
      "2: 观测 = [-0.03068302 -0.21177897 -0.01603174  0.29268365], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "3: 动作 = 1\n",
      "3: 观测 = [-0.0349186  -0.01643215 -0.01017807 -0.00501206], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "4: 动作 = 1\n",
      "4: 观测 = [-0.03524724  0.17883428 -0.01027831 -0.30088883], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "5: 动作 = 0\n",
      "5: 观测 = [-0.03167055 -0.01613968 -0.01629609 -0.01146511], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "6: 动作 = 0\n",
      "6: 观测 = [-0.03199335 -0.21102418 -0.01652539  0.27603198], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "7: 动作 = 1\n",
      "7: 观测 = [-0.03621383 -0.0156704  -0.01100475 -0.02181691], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "8: 动作 = 1\n",
      "8: 观测 = [-0.03652724  0.17960762 -0.01144109 -0.31795155], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "9: 动作 = 0\n",
      "9: 观测 = [-0.03293509 -0.01534952 -0.01780012 -0.02889858], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "10: 动作 = 0\n",
      "10: 观测 = [-0.03324208 -0.21021174 -0.01837809  0.25811549], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "11: 动作 = 1\n",
      "11: 观测 = [-0.03744631 -0.01483231 -0.01321578 -0.04030703], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "12: 动作 = 1\n",
      "12: 观测 = [-0.03774296  0.18047664 -0.01402192 -0.33713019], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "13: 动作 = 1\n",
      "13: 观测 = [-0.03413342  0.3757953  -0.02076453 -0.63420168], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "14: 动作 = 1\n",
      "14: 观测 = [-0.02661752  0.57120064 -0.03344856 -0.93335085], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "15: 动作 = 0\n",
      "15: 观测 = [-0.01519351  0.37654554 -0.05211558 -0.65136356], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "16: 动作 = 0\n",
      "16: 观测 = [-0.00766259  0.18218668 -0.06514285 -0.37553614], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "17: 动作 = 1\n",
      "17: 观测 = [-0.00401886  0.37817051 -0.07265357 -0.6880268 ], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "18: 动作 = 1\n",
      "18: 观测 = [ 0.00354455  0.57422164 -0.08641411 -1.00266906], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "19: 动作 = 0\n",
      "19: 观测 = [ 0.01502898  0.38035385 -0.10646749 -0.7383266 ], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "20: 动作 = 0\n",
      "20: 观测 = [ 0.02263606  0.18685066 -0.12123402 -0.48095767], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "21: 动作 = 0\n",
      "21: 观测 = [ 0.02637307 -0.00637012 -0.13085317 -0.22880938], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "22: 动作 = 0\n",
      "22: 观测 = [ 0.02624567 -0.19940299 -0.13542936  0.01990341], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "23: 动作 = 0\n",
      "23: 观测 = [ 0.02225761 -0.39234904 -0.13503129  0.26697777], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "24: 动作 = 1\n",
      "24: 观测 = [ 0.01441063 -0.19558428 -0.12969174 -0.06506134], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "25: 动作 = 1\n",
      "25: 观测 = [ 0.01049894  0.00113557 -0.13099297 -0.39568555], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "26: 动作 = 0\n",
      "26: 观测 = [ 0.01052166 -0.19190813 -0.13890668 -0.14700269], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "27: 动作 = 0\n",
      "27: 观测 = [ 0.00668349 -0.38479582 -0.14184673  0.09883464], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "28: 动作 = 1\n",
      "28: 观测 = [-0.00101242 -0.18795596 -0.13987004 -0.23502297], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "29: 动作 = 0\n",
      "29: 观测 = [-0.00477154 -0.38083124 -0.1445705   0.01047671], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "30: 动作 = 0\n",
      "30: 观测 = [-0.01238817 -0.57361584 -0.14436096  0.25428107], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "31: 动作 = 0\n",
      "31: 观测 = [-0.02386048 -0.76641321 -0.13927534  0.49817405], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "32: 动作 = 1\n",
      "32: 观测 = [-0.03918875 -0.56963069 -0.12931186  0.16504378], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "33: 动作 = 1\n",
      "33: 观测 = [-0.05058136 -0.37291759 -0.12601098 -0.16547411], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "34: 动作 = 1\n",
      "34: 观测 = [-0.05803971 -0.1762382  -0.12932047 -0.4951033 ], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "35: 动作 = 0\n",
      "35: 观测 = [-0.06156448 -0.36932199 -0.13922253 -0.24581038], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "36: 动作 = 0\n",
      "36: 观测 = [-6.89509179e-02 -5.62209366e-01 -1.44138740e-01 -7.81575773e-05], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "37: 动作 = 0\n",
      "37: 观测 = [-0.08019511 -0.75500161 -0.1441403   0.24388112], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "38: 动作 = 0\n",
      "38: 观测 = [-0.09529514 -0.94780215 -0.13926268  0.48785219], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "39: 动作 = 1\n",
      "39: 观测 = [-0.11425118 -0.75101838 -0.12950564  0.15472322], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "40: 动作 = 0\n",
      "40: 观测 = [-0.12927155 -0.94407115 -0.12641117  0.40390908], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "41: 动作 = 1\n",
      "41: 观测 = [-0.14815297 -0.74740433 -0.11833299  0.07419675], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "42: 动作 = 0\n",
      "42: 观测 = [-0.16310106 -0.94064857 -0.11684906  0.32732726], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "43: 动作 = 1\n",
      "43: 观测 = [-1.81914029e-01 -7.44073725e-01 -1.10302511e-01  2.00185542e-04], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "44: 动作 = 0\n",
      "44: 观测 = [-0.1967955  -0.93745512 -0.11029851  0.25614624], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "45: 动作 = 1\n",
      "45: 观测 = [-0.21554461 -0.74094539 -0.10517558 -0.0691892 ], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "46: 动作 = 0\n",
      "46: 观测 = [-0.23036351 -0.93441454 -0.10655937  0.18854626], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "47: 动作 = 1\n",
      "47: 观测 = [-0.2490518  -0.73794222 -0.10278844 -0.13575981], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "48: 动作 = 0\n",
      "48: 观测 = [-0.26381065 -0.93145317 -0.10550364  0.12280795], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "49: 动作 = 1\n",
      "49: 观测 = [-0.28243971 -0.73499031 -0.10304748 -0.20120829], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "50: 动作 = 1\n",
      "50: 观测 = [-0.29713952 -0.5385571  -0.10707164 -0.52453745], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "51: 动作 = 0\n",
      "51: 观测 = [-0.30791066 -0.73202221 -0.11756239 -0.26742061], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "52: 动作 = 1\n",
      "52: 观测 = [-0.3225511  -0.53543581 -0.12291081 -0.59474859], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "53: 动作 = 0\n",
      "53: 观测 = [-0.33325982 -0.72864255 -0.13480578 -0.34316967], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "54: 动作 = 1\n",
      "54: 观测 = [-0.34783267 -0.53188612 -0.14166917 -0.67513967], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "55: 动作 = 1\n",
      "55: 观测 = [-0.35847039 -0.33510933 -0.15517196 -1.00885934], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "56: 动作 = 0\n",
      "56: 观测 = [-0.36517258 -0.52785803 -0.17534915 -0.76864782], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "57: 动作 = 1\n",
      "57: 观测 = [-0.37572974 -0.3308119  -0.19072211 -1.11097354], 奖励 = 1.0, 结束标识 = False, 其它信息 = {}\n",
      "58: 动作 = 1\n",
      "58: 观测 = [-0.38234598 -0.13376695 -0.21294158 -1.45691459], 奖励 = 1.0, 结束标识 = True, 其它信息 = {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()  # 幕的初始状态\n",
    "print(\"初始观测：{}\".format(observation))\n",
    "for i in range(200):\n",
    "    env.render()  # 帧渲染, 决定了是否显示游戏界面\n",
    "    action = env.action_space.sample()  # 随机选择一个动作\n",
    "    print(\"{}: 动作 = {}\".format(i, action))\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(\"{}: 观测 = {}, 奖励 = {}, 结束标识 = {}, 其它信息 = {}\".format(i, obs, reward, done, info))\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 玩多局游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1局得分: 11.0\n",
      "第2局得分: 12.0\n",
      "第3局得分: 12.0\n",
      "第4局得分: 12.0\n",
      "第5局得分: 24.0\n",
      "第6局得分: 16.0\n",
      "第7局得分: 25.0\n",
      "第8局得分: 21.0\n",
      "第9局得分: 18.0\n",
      "第10局得分: 16.0\n",
      "第11局得分: 16.0\n",
      "第12局得分: 11.0\n",
      "第13局得分: 32.0\n",
      "第14局得分: 13.0\n",
      "第15局得分: 15.0\n",
      "第16局得分: 16.0\n",
      "第17局得分: 15.0\n",
      "第18局得分: 11.0\n",
      "第19局得分: 24.0\n",
      "第20局得分: 11.0\n"
     ]
    }
   ],
   "source": [
    "episode_num = 20\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "for e in range(episode_num):\n",
    "    obs = env.reset()\n",
    "    episode_rewards = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        obs = obs\n",
    "        if done:\n",
    "            print(\"第{}局得分: {}\".format(e + 1, episode_rewards))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 使用 DQN (FNN+Adam) 训练游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤 1：构建 FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "fnn = nn.Sequential(\n",
    "    nn.Linear(env.observation_space.shape[0], 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, env.action_space.n)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤 2：决策方法 ($\\epsilon$-greedy 策略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def act(net, state, epsilon):\n",
    "    if random.random() > epsilon:  # 选择Q值最大的动作\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_value = net.forward(state)  # torch.Size([1, 2])\n",
    "        action = q_value.max(1)[1].item()\n",
    "    else:  # 随机选择一个动作\n",
    "        action = random.randrange(env.action_space.n)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤 3：$\\epsilon$ 随时间衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_epsilon(t, epsilon_start=1.0, epsilon_final=0.01, epsilon_decay=500):\n",
    "    epsilon = epsilon_final + (epsilon_start-epsilon_final) * math.exp(-1. * t/epsilon_decay)\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤 4：定义经验回放集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(* random.sample(self.buffer, batch_size))\n",
    "        state = np.concatenate(state)\n",
    "        next_state = np.concatenate(next_state)\n",
    "        \n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤 5：训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1局收益 = 12.0\n",
      "第2局收益 = 33.0\n",
      "第3局收益 = 18.0\n",
      "第4局收益 = 33.0\n",
      "第5局收益 = 60.0\n",
      "第6局收益 = 22.0\n",
      "第7局收益 = 15.0\n",
      "第8局收益 = 26.0\n",
      "第9局收益 = 37.0\n",
      "第10局收益 = 66.0\n",
      "第11局收益 = 38.0\n",
      "第12局收益 = 127.0\n",
      "第13局收益 = 74.0\n",
      "第14局收益 = 200.0\n",
      "第15局收益 = 66.0\n",
      "第16局收益 = 60.0\n",
      "第17局收益 = 95.0\n",
      "第18局收益 = 200.0\n",
      "第19局收益 = 200.0\n",
      "第20局收益 = 200.0\n",
      "第21局收益 = 131.0\n",
      "第22局收益 = 200.0\n",
      "第23局收益 = 200.0\n",
      "第24局收益 = 200.0\n",
      "第25局收益 = 200.0\n",
      "第26局收益 = 200.0\n",
      "第27局收益 = 200.0\n",
      "第28局收益 = 200.0\n",
      "第29局收益 = 200.0\n",
      "第30局收益 = 200.0\n",
      "第31局收益 = 200.0\n",
      "第32局收益 = 200.0\n",
      "第33局收益 = 200.0\n",
      "第34局收益 = 200.0\n",
      "第35局收益 = 200.0\n",
      "第36局收益 = 200.0\n",
      "第37局收益 = 200.0\n"
     ]
    }
   ],
   "source": [
    "import torch.optim\n",
    "optimizer = torch.optim.Adam(fnn.parameters())\n",
    "\n",
    "t = 0  # 训练步数,用于计算epsilon\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "episode_rewards = []  # 各局得分,用来判断训练是否完成\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # 开始新的一局\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    while True:\n",
    "        epsilon = calc_epsilon(t)\n",
    "        action = act(fnn, state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.store(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            sample_state, sample_action, sample_reward, sample_next_state, \\\n",
    "                    sample_done = replay_buffer.sample(batch_size)\n",
    "            sample_state = torch.tensor(sample_state, dtype=torch.float32)\n",
    "            sample_action = torch.tensor(sample_action, dtype=torch.int64)\n",
    "            sample_reward = torch.tensor(sample_reward, dtype=torch.float32)\n",
    "            sample_next_state = torch.tensor(sample_next_state,\n",
    "                    dtype=torch.float32)\n",
    "            sample_done = torch.tensor(sample_done, dtype=torch.float32)\n",
    "            \n",
    "            next_qs = fnn(sample_next_state)\n",
    "            next_q= next_qs.max(1)[0]\n",
    "            expected_q = sample_reward + gamma * next_q * (1 - sample_done)\n",
    "            \n",
    "            qs = fnn(sample_state)\n",
    "            q = qs.gather(1, sample_action.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            td_error = expected_q - q\n",
    "            # 计算 MSE 损失\n",
    "            loss = td_error.pow(2).mean() \n",
    "            \n",
    "            # 根据损失改进网络\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "        if done: # 本局结束\n",
    "            episode_rewards.append(episode_reward)\n",
    "            i_episode = len(episode_rewards)\n",
    "            print ('第{}局收益 = {}'.format(i_episode, episode_reward))\n",
    "            break\n",
    "            \n",
    "    if len(episode_rewards) > 20 and np.mean(episode_rewards[-20:]) > 195:\n",
    "        break # 训练结束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤 6：利用训练好的游戏 AI 玩游戏 (贪婪策略，$\\epsilon = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1局收益 = 200.0\n",
      "第2局收益 = 200.0\n",
      "第3局收益 = 200.0\n",
      "第4局收益 = 200.0\n",
      "第5局收益 = 200.0\n",
      "第6局收益 = 200.0\n",
      "第7局收益 = 200.0\n",
      "第8局收益 = 200.0\n",
      "第9局收益 = 200.0\n",
      "第10局收益 = 200.0\n",
      "第11局收益 = 200.0\n",
      "第12局收益 = 200.0\n",
      "第13局收益 = 200.0\n",
      "第14局收益 = 200.0\n",
      "第15局收益 = 200.0\n",
      "第16局收益 = 200.0\n",
      "第17局收益 = 200.0\n",
      "第18局收益 = 200.0\n",
      "第19局收益 = 200.0\n",
      "第20局收益 = 200.0\n"
     ]
    }
   ],
   "source": [
    "n_episode = 20 \n",
    "for e in range(n_episode):\n",
    "    observation = env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action  = act(fnn, observation, 0)  # 贪婪策略\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        state = observation\n",
    "        if done:\n",
    "            break\n",
    "    print ('第{}局收益 = {}'.format(e+1, episode_reward))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
